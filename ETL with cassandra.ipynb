{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling with Apache Cassandra\n",
    "\n",
    "I created this project with the goal to create an Apache Cassandra database to run some analysis in songs data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "CSV_EVENTS_FILE = 'event_datafile_new.csv'\n",
    "KEYSPACE = 'anobrega'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):    \n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_rows_list = [] \n",
    "\n",
    "for f in file_path_list:\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open(CSV_EVENTS_FILE, 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','iteminsession','lastName','length',\\\n",
    "                'level','location','sessionid','song','userid'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the number of rows in csv file and viewing the columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines 6820\n",
      "Index(['artist', 'firstName', 'gender', 'iteminsession', 'lastName', 'length',\n",
      "       'level', 'location', 'sessionid', 'song', 'userid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_events = pd.read_csv(CSV_EVENTS_FILE, sep=',')\n",
    "print(f\"Number of lines {len(df_events)}\")\n",
    "print(df_events.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeSafe(func):\n",
    "    \"\"\"Try to execute function passed, if it fails print error\"\"\"\n",
    "    try:\n",
    "        return func()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def execute_query_without_return(session, query):\n",
    "    \"\"\"Execute query in a safe way\"\"\"\n",
    "    executeSafe(lambda: session.execute(query))\n",
    "\n",
    "\n",
    "def insert_rows_from_file(session, insert_query, positions_mapper, file=CSV_EVENTS_FILE):\n",
    "    \"\"\"Reads the file passed, map insert query and execute it\"\"\"\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        next(csvreader)\n",
    "        for line in csvreader:\n",
    "            session.execute(insert_query, (tuple([line[p[0]] if not p[1] else p[1](\n",
    "                line[p[0]]) for p in positions_mapper.items()])))\n",
    "\n",
    "\n",
    "def validate_insert(session, table_name):\n",
    "    \"\"\"Validate if the data was inserted\"\"\"\n",
    "    validate_first_insert_query = f\"SELECT count(*) FROM {table_name}\"\n",
    "    rows = executeSafe(lambda: session.execute(validate_first_insert_query))\n",
    "    print(\"############## INSERTED LINES ##############\")\n",
    "    for item in rows:\n",
    "        print(f\"Number of lines in {table_name}: {item.count}\")\n",
    "    print(\"############################################\")\n",
    "\n",
    "\n",
    "def view_schema(cluster, table_name, keyspace=KEYSPACE):\n",
    "    \"\"\"Shows the table schema\"\"\"\n",
    "    print(\"############## SCHEMA ##############\")\n",
    "    print(\n",
    "        cluster.metadata.keyspaces[keyspace].tables[table_name].export_as_string())\n",
    "    print(\"####################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to local Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = executeSafe(lambda: Cluster([\"127.0.0.1\"]))\n",
    "\n",
    "session = executeSafe(lambda: cluster.connect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and setting keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyspace_create_query = f\"CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\" + \\\n",
    "    \" WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1}\"\n",
    "\n",
    "execute_query_without_return(session, keyspace_create_query)\n",
    "\n",
    "executeSafe(lambda: session.set_keyspace(KEYSPACE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Getting artist, song title and length informations history filtering by session and iteminsession\n",
    "\n",
    "Since we want to filter by `sessionid` and `iteminsession` they must be the composite key, in this sequence. So the data will be ordered by `iteminsession`.\n",
    "\n",
    "#### Creating table 'song_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## SCHEMA ##############\n",
      "CREATE TABLE anobrega.song_info (\n",
      "    sessionid int,\n",
      "    iteminsession int,\n",
      "    artist text,\n",
      "    song_length float,\n",
      "    song_title text,\n",
      "    PRIMARY KEY (sessionid, iteminsession)\n",
      ") WITH CLUSTERING ORDER BY (iteminsession ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "first_table_name = 'song_info'\n",
    "create_table_query1 = f\"CREATE TABLE {first_table_name} (sessionid int, iteminsession int, artist text, song_title text, song_length float, PRIMARY KEY(sessionid, iteminsession)) \"\n",
    "execute_query_without_return(session, create_table_query1)\n",
    "\n",
    "view_schema(cluster, first_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping and inserting data into the new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## INSERTED LINES ##############\n",
      "Number of lines in song_info: 6820\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "insert_query = f\"INSERT INTO {first_table_name} (sessionid, iteminsession, artist, song_title, song_length)\"\n",
    "insert_query = insert_query + \" VALUES (%s, %s, %s, %s, %s);\"\n",
    "mapper_table1 = { 8: int, 3: int, 0: None, 9: None, 5: float}\n",
    "insert_rows_from_file(session, insert_query, mapper_table1)\n",
    "\n",
    "validate_insert(session, first_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 The first analyse that we'll do is to find out what are the records with `sessionid` = 338 and  `iteminsession` = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless, Music Matters (Mark Knight Dub), 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "first_query = f\"SELECT artist, song_title, song_length FROM {first_table_name} WHERE sessionid = 338 and iteminsession = 4;\"\n",
    "\n",
    "rows = executeSafe(lambda : session.execute(first_query))\n",
    "for item in rows:\n",
    "    print(f\"{item.artist}, {item.song_title}, {item.song_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Getting artist, song and user informations by `userid` and `sessionid` sorting by `iteminsession`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to filter by `userid` and `sessionid` they have to be considered as the composite key. And, in this case, we want to sort data by `iteminsession`, then this must be the clustering key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling 'song_user_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## SCHEMA ##############\n",
      "CREATE TABLE anobrega.song_user_info (\n",
      "    userid int,\n",
      "    sessionid int,\n",
      "    iteminsession int,\n",
      "    artist text,\n",
      "    song_title text,\n",
      "    user_first_name text,\n",
      "    user_last_name text,\n",
      "    PRIMARY KEY ((userid, sessionid), iteminsession)\n",
      ") WITH CLUSTERING ORDER BY (iteminsession ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "second_table_name = 'song_user_info'\n",
    "create_table_query2 = f\"CREATE TABLE {second_table_name} (userid int, iteminsession int, sessionid int, artist text, song_title text, user_first_name text, user_last_name text, PRIMARY KEY((userid, sessionid), iteminsession))\"\n",
    "execute_query_without_return(session, create_table_query2)\n",
    "view_schema(cluster, second_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping and inserting data into the new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## INSERTED LINES ##############\n",
      "Number of lines in song_user_info: 6820\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "insert_query2 = f\"INSERT INTO {second_table_name} (artist, song_title, userid, user_first_name, user_last_name, sessionid, iteminsession) \"\n",
    "insert_query2 = insert_query2 + \" VALUES (%s, %s, %s, %s, %s, %s, %s);\"\n",
    "mapper_table2 = {0: str, 9: str, 10: int, 1: str, 4: str, 8: int, 3: int}\n",
    "insert_rows_from_file(session, insert_query2, mapper_table2)\n",
    "\n",
    "validate_insert(session, second_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 In this second analyse we want the `artist`, `song_title`, `user first and last name` that matches by `userid` = 10 and `sessionid` = 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down To The Bone, Keep On Keepin' On, Sylvie Cruz\n",
      "Three Drives, Greece 2000, Sylvie Cruz\n",
      "Sebastien Tellier, Kilometer, Sylvie Cruz\n",
      "Lonnie Gordon, Catch You Baby (Steve Pitron & Max Sanna Radio Edit), Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "second_query = f\"SELECT artist, song_title, user_first_name, user_last_name FROM {second_table_name} WHERE userid = 10 AND sessionid = 182\"\n",
    "\n",
    "rows = executeSafe(lambda: session.execute(second_query))\n",
    "\n",
    "for item in rows:\n",
    "    print(f\"{item.artist}, {item.song_title}, {item.user_first_name} {item.user_last_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Getting `song title`, `user first and last name` who listened determined song\n",
    "\n",
    "In this case we need to filter data by the `song title`, so it must be our PK. But to avoid that some data be supressed we also need a unique information to make a composite key, `userid` will serve for this, since we want the users information too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling 'user_song_listen_history' for retrieve this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## SCHEMA ##############\n",
      "CREATE TABLE anobrega.user_song_listen_history (\n",
      "    song_title text,\n",
      "    userid int,\n",
      "    user_first_name text,\n",
      "    user_last_name text,\n",
      "    PRIMARY KEY (song_title, userid)\n",
      ") WITH CLUSTERING ORDER BY (userid ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "third_table_name = 'user_song_listen_history'\n",
    "create_table_query3 = f\"CREATE TABLE {third_table_name} (song_title text, userid int, user_first_name text, user_last_name text, PRIMARY KEY((song_title), userid));\"\n",
    "execute_query_without_return(session, create_table_query3)\n",
    "\n",
    "view_schema(cluster, third_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting data from csv file for user_song_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## INSERTED LINES ##############\n",
      "Number of lines in user_song_listen_history: 6618\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "insert_query3 = f\"INSERT INTO {third_table_name} (song_title, userid, user_first_name, user_last_name) \"\n",
    "insert_query3 = insert_query3 + \" VALUES (%s, %s, %s, %s);\"\n",
    "mapper_table3 = {9: str, 10: int, 1: str, 4: str}\n",
    "insert_rows_from_file(session, insert_query3, mapper_table3)\n",
    "\n",
    "validate_insert(session, third_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 We are going to list all users that listened to music 'All Hands Against His Own'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "third_query = f\"SELECT user_first_name, user_last_name FROM {third_table_name} WHERE song_title = 'All Hands Against His Own'\"\n",
    "\n",
    "rows = executeSafe(lambda : session.execute(third_query))\n",
    "\n",
    "for item in rows:\n",
    "    print(f\"{item.user_first_name} {item.user_last_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in [first_table_name, second_table_name, third_table_name]:\n",
    "    execute_query_without_return(session, f\"DROP TABLE IF EXISTS {table_name};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutting down session and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0670ebc67bce8a62c9a76963933642bfc6dab4a791b239447a0885ce70960d94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
